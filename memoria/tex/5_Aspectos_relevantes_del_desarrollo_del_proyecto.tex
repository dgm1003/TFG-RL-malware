\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}
En este capítulo se recogen los aspectos más importantes del proceso de desarrollo de este proyecto, cubriendo desde los procesos de selección de tema y planificación hasta los problemas que surgieron durante el desarrollo, y explicando por qué se tomaron las decisiones que se tomaron.


\section{Inicio del proyecto}

La idea del proyecto surgió del trabajo de uno de los tutores en el área del aprendizaje por refuerzo, y otros trabajos fin de grado en los que había participado anteriormente, y el alumno se interesó en dicha idea, pues tanto el aprendizaje por refuerzo como la propagación de malware y el hacking ético son áreas que no se suelen tratar en la carrera pero están ganando cada vez más importancia en el ámbito profesional.

Originalmente, el trabajo se propuso con la idea de utilizar aprendizaje profundo por refuerzo, pero rápidamente se llegó a la conclusión de que sería mejor dejar el aprendizaje profundo a un lado y centrarse en el aprendizaje por refuerzo -- para empezar desde cero y comprender mejor los elementos involucrados y cómo afectan al contexto seleccionado -- dejando así el uso de aprendizaje profundo para una posible ampliación, como un trabajo fin de máster o una continuación del proyecto fuera de la universidad.

\section{Primeros pasos y obtención de conocimientos}

Después de haber decidido el tipo de aprendizaje que se utilizaría en el proyecto, y después de unas reuniones iniciales, se definieron los objetivos que se quería alcanzar más allá de la idea inicial, como el diseño de una visualización de los resultados y una interfaz en modo de página web, se seleccionaron los programas y metodologías a utilizar a lo largo del proyecto, y se pasó a la fase de formación y obtención de conocimientos. Estos dos primeros apartados se muestran en más profundidad en las secciones de ``Objetivos del proyecto'' y ``Técnicas y herramientas'' respectivamente.

En cuanto a la formación, para la realización del proyecto se necesitaban conocimientos no solamente sobre el aprendizaje por refuerzo y las diferentes variedades que se podrían implementar, sino también sobre qué librerías de python servirían para programar dicho aprendizaje y cómo funcionan, además de los elementos y topologías de redes de ordenadores.

Los recursos utilizados a la hora de ampliar mis conocimientos sobre el aprendizaje por refuerzo y las redes de ordenadores fueron:
\begin{itemize}
    \item Libro \textit{Reinforcement Learning: An Introduction} (Richard S.Sutton y Andrew G. Barto)\cite{RLBook}
    \item Presentación \textit{Descubriendo información en una red compleja usando aprendizaje por refuerzo}\\ \url{https://youtu.be/vu3rruSrtTo}
    \item Curso de DeepMind y UCL \textit{Introduction to Reinforcement Learning with David Silver}\cite{DeepMind}
    \item Artículo \textit{Playing Atari with Deep Reinforcement Learning} (Volodymyr Mnih, Koray Kavukcuoglu et al)\cite{AtariDRL}
    \item Seminarios ofrecidos por los tutores.
\end{itemize}

Los tutoriales que se siguieron a la hora de aprender las diferentes librerías de python que se pretendía utilizar fueron los enumerados a continuación. No todos se llegaron a realizar al completo, pero al menos fueron leídos y sirvieron para entender algunos de los conceptos:
\begin{itemize}
    \item Numpy:
    \begin{itemize}
        \item Primera toma de contacto, ejemplo simple: \\
        \url{http://firsttimeprogrammer.blogspot.com/2016/09/getting-ai-smarter-with-q-learning.html}
        \item Situación más compleja, junto con varias explicaciones de conceptos:\\ \url{https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/}
        \item  Otra situación:\\ \url{https://www.analyticsvidhya.com/blog/2021/04/q-learning-algorithm-with-step-by-step-implementation-using-python/}
        \item Ejemplo de uso de grafos para aprendizaje por refuerzo:\\ \url{https://www.viralml.com/video-content.html?v=nSxaG_Kjw_w}
    \end{itemize}
    \item Gym, Keras:
    \begin{itemize}
        \item  Ejemplo simple de uso en Q-learning:\\ \url{https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/}
        \item Tutoriales de tanto aprendizaje por refuerzo como aprendizaje profundo por refuerzo: \\
        \url{https://rubikscode.net/2021/07/13/deep-q-learning-with-python-and-tensorflow-2-0/}
        \item Tutorial sobre entornos de Gym: \\
        \url{https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/5_custom_gym_env.ipynb}
        \item Vídeo de Nicholas Renotte sobre problema de CartPole de Gym, versión resumida de 20 minutos: \\ \url{https://www.youtube.com/watch?v=cO5g5qLrLSo}
        \item Curso completo de Nicholas Renotte de aprendizaje por refuerzo en 3 horas: \\ \url{https://www.youtube.com/watch?v=Mut_u40Sqz4}
        \item Vídeo de Nicholas Renotte sobre entornos de gym: \\
        \url{https://www.youtube.com/watch?v=bD6V3rcr_54}
        \item Otro tutorial del entorno de CartPole de Gym: \\
        \url{https://keon.github.io/deep-q-learning/}
    \end{itemize}
    \item NetworkX:
    \begin{itemize}
        \item Tutorial de la documentación: \\ \url{https://networkx.org/documentation/stable/tutorial.html}
    \end{itemize}
\end{itemize}

\section{Diseño del problema}
Una vez se tenía los suficientes conocimientos como para empezar a diseñar el problema, se empezó a realizar tanto el aprendizaje como el desarrollo en tándem. También se fueron proponiendo puntos en los que sería posible la ampliación del alcance del problema, aunque no se fuesen a desarrollar dentro del proyecto.

\subsection{Problema}
El problema se definió de la siguiente manera:

El usuario tendría conocimiento de una red de ordenadores, gracias a un proceso anterior a la introducción del malware a dicha red. De esta red, se conocerían las interconexiones entre los diferentes dispositivos, además de una estimación de como de seguro sería cada uno de los dispositivos (obtenida a partir de las características de cada dispositivo, como el tipo de dispositivo, modelo, o importancia dentro de la empresa). Esta estimación de la seguridad de un dispositivo, por lo tanto, sería proporcional a la probabilidad de que el malware fuera detectado si intentase desplazarse por ese dispositivo. Por lo tanto, se tendría un nivel de información parcial sobre el entorno en el que actuaría el agente. Gracias a esta información sobre la red, el informático que fuese a liberar al malware en el sistema podría definir un dispositivo como objetivo para la infección.

De este modo, se daría por completada la misión del malware si, una vez entrase en la red por algún método, consiguiese acceder al dispositivo objetivo e infectarlo sin ser detectado. 

Para poder obtener resultados satisfactorios, se optó por realizar el enrutado mediante un algoritmo de aprendizaje por refuerzo, de modo que aprendiese las características de la red y encontrase la ruta más corta y segura hasta su destino. Para el entrenamiento del agente se decidió utilizar un método de tablas Q.

Para que este algoritmo obtuviese los resultados esperados, se definió un conjunto de estados, acciones y recompensas, que se explican en profundidad en el anexo C.

\subsection{Red}
La red se decidió representar como un grafo, siendo los nodos los diferentes dispositivos, permitiendo el entrenamiento del algoritmo con diferentes redes y topologías, sin afectar a su rendimiento. Para este trabajo, se tomará una visión simplificada, definiendo ciertas restricciones. Se representarán en forma de grafos, siendo los nodos los diferentes dispositivos y las aristas las conexiones entre dichos dispositivos.

Por un lado asumiremos que, entre dos dispositivos concretos, solamente habrá una conexión directa única, y no se tendrá en cuenta el tipo de conexión (dará igual que sea por un cable USB entre un ordenador y un teléfono móvil, o mediante conexión inalámbrica a un router, o cable ethernet a un switch). Lo que le importará a nuestro algoritmo no será de qué modo moverse entre dos nodos conectados, sino decidir por qué nodos pasar en su recorrido hasta el objetivo.

Por otro lado, los nodos podrán ser una gran variedad de dispositivos: tanto routers con acceso a otras redes, como switches (con o sin firewall), además de ordenadores de sobremesa y portátiles, impresoras, escáneres, teléfonos móviles, u otros dispositivos conectados a la red. Sin embargo, el algoritmo no decidirá en base a qué tipo de dispositivo sea cada nodo, sino en base a dos criterios:
\begin{itemize}
    \item \textbf{Si es un nodo hoja o no}: si solamente tiene una conexión (es una hoja), desplazarse a ese nodo no aportará nada al algoritmo, pues tendrá que retroceder para poder continuar su camino hacia el objetivo. Se preferirán nodos con más de una conexión, sin importar el número mientras sea mayor de uno.
    \item \textbf{Grado de seguridad del nodo}: dispositivos como ordenadores sobremesa de posiciones críticas o switches con firewalls tendrán un mayor número de medidas de seguridad, que puedan detectar al malware y realizar acciones para detenerlo. Por lo tanto, el algoritmo intentará evitar dichos dispositivos.
\end{itemize}
Estos dos criterios serán los atributos de cada nodo, representados como un valor numérico.

Las redes se generarán de forma relativamente aleatoria, una vez se definan varios parámetros como el número de nodos de la red, para probar el algoritmo en una gran cantidad de situaciones. Se podrá probar con diferentes topologías, como topologías de árbol o en anillo. Para la generación de redes se hará uso de librerías de Python, las cuales se pueden ver en el apartado de Técnicas y Herramientas.

\section{Desarrollo del algoritmo}

Siguiendo la metodología SCRUM, se enfocó el programa de modo que se empezase con una versión simple pero funcional del código, y en cada iteración se iría aumentando el número de características, o mejorando su funcionamiento. Se pasó por las siguientes iteraciones:

\begin{enumerate}
    \item \underline{Versión inicial:} Se comenzó desarrollando un producto mínimo viable, que encontrase una solución para una red relativamente simple, comprobando que se había entendido cómo programar una instancia del aprendizaje por refuerzo. Consistía en el código necesario para poder definir la red y entrenar un agente hasta que encontrase una ruta hasta el objetivo.

    \item \underline{División en estructura de clases:} A continuación, se aplicaron los conocimientos obtenidos en el grado respecto a la Ingeniería de Software y Programación Orientada a Objetos para dividir el código inicial en un conjunto de funciones, incluidas en una clase. De este modo, la utilización del programa se simplificó considerablemente, pues el usuario tendría que crear una nueva instancia de la clase y llamar a unas pocas funciones, en vez de buscar diferentes valores en el propio código y modificarlos. Además, la transformación del código en una clase permitía también que la implementación en otros contextos fuese más fácil.

    \item \underline{Función de recompensa:} Más adelante, se consideró que, en vez de obtener los valores de recompensa desde una tabla de $2n*2n$ posiciones (siendo $n$ el número de nodos de la red), sería más eficiente obtener dicha recompensa de una función que tomase como entrada el estado actual y la acción a realizar. De este modo, no solo se ahorraría espacio en memoria (el cual sería considerable con redes grandes), sino que también, aunque ejecutar la función tardase más tiempo que acceder a la correspondiente posición de la tabla, se ahorraría el tiempo necesario en construir la tabla completa. Además, resulta en un método más fácil de entender de cara a nuevos programadores que decidan trabajar en el proyecto en el futuro.

    \item \underline{Implementación de NetworkX:} El problema con las versiones anteriores surgía de que, para definir la red de ordenadores, se utilizaba un vector con tuplas de dos números, cada tupla representando una conexión entre dos nodos. Esto resultaba muy engorroso a la hora de definir redes con números elevados de nodos y conexiones, por lo que se pasó a utilizar una estructura de grafos para representar dicha red. Se optó por la librería de NetworkX, pues era relativamente fácil de aprender, pero ofrecía una gran cantidad de opciones a la hora de generar y manipular grafos. Después de un breve periodo de aprendizaje, y de decidir en la forma de generación de dichos grafos, se implementó la librería en el código, sustituyendo a la antigua representación de las redes.

    \item \underline{Separación de algoritmo y red:} La situación que surgió al incluir NetworkX era que el código del algortimo estaba inevitablemente relacionado con la representación de la red, cosa que se hizo evidente al ver la cantidad de cambios que hubo que realizar para adaptar el uso de Networkx para nuestro algoritmo. Por ello, el siguiente paso consistió en separar la programación del algoritmo de la representación de la red, de modo que si se quisiera cambiar uno de los dos elementos en algún momento, sería mucho menos probable que hubiese que modificar el otro elemento. Esto aumentó la posible reutilización del código en otros contextos de manera considerable, además de simplificar su mantenimiento de cara a otros programadores que fueran a utilizarlo.

    \item \underline{Separación entre agente y entorno:} Expandiendo en la idea anterior, se decidió ir un paso más allá, separando el agente del entorno. De este modo, el agente no sería el responsable de modificar la situación de los componentes de la red, ni obtener la recompensa de una acción ni otras cuestiones similares, delegando todas estas tareas al entorno (además de la gestión de la red como se explica en el apartado anterior), y centrándose solamente en entrenar su modelo (o tabla Q), y en buscar las rutas óptimas.

    \item \underline{Entrenamiento mediante episodios:} Esta separación entre agente y entorno dio lugar a más oportunidades de expansión. La que se consideró más interesante para este trabajo fue el cambio a entrenar mediante episodios.  
    
    \item \underline{Más opciones para nodos de riesgo:} Por último, se realizaron mejoras varias, enriqueciendo la representación gráfica del entorno e incluyendo la posibilidad de redefinir los nodos de riesgo.
    
\end{enumerate}


\section{Desarrollo de la página web}

Para la creación de la página web, se optó por diseñarla utilizando Flask, e incluirla dentro de una imagen de Docker para facilitar su uso e instalación. Estas herramientas fueron seleccionadas debido a la familiaridad que se tenía con ellas, además de que se adaptaban a las necesidades del proyecto.

Originalmente se quiso crear una aplicación Docker, que se preparase con el comando docker-compose, simplificando el trabajo de los usuarios. Sin embargo, debido a varios problemas que se encontraron durante el desarrollo, se acabó descartando esta opción por una más simple, que consistía en crear solamente una imagen Docker, y pedir a los usuarios que la construyeran y ejecutasen a continuación un contenedor con ella. 

En cuanto a la estructura, se decidió dejar el código del algoritmo separado del código de la página web. Para ello, se dividieron en backend y frontend, respectivamente.

\subsection{Diseño del backend}
El backend consistía simplemente en los dos ficheros python del agente y del entorno. Sin embargo, se realizaron algunos cambios y adiciones al código para adaptarlo a las necesidades de la página web. En el agente no hubo modificaciones, excepto la eliminación del método main, al no ser necesario. Sin embargo, en el entorno sí que hubo varias diferencias importantes. 

\begin{itemize}
    \item La primera adición fue la posibilidad de crear un entorno sin especificar los nodos inicial y final. Como consecuencia, se tuvo que crear un nuevo método para asignar inicio y meta al entorno. Este cambio ocurrió porque se quería que el usuario de la página seleccionase el origen y destino una vez hubiese visualizado la red de ordenadores que se había generado.

    \item Por otro lado, se añadió la opción de guardar el renderizado del entorno como una imagen en una carpeta determinada. De ese modo, se podrían mostrar las imágenes más adelante en la página web de forma mucho más sencilla. 
    
    \item Otra nueva funcionalidad que se añadió fue la opción de transformar al entorno a un formato JSON, y viceversa, para poder guardarlo como variable de sesión en la página web. Esto conllevó a tener que crear dos métodos nuevos, además de modificar el constructor de la clase.
    
    \item Por último, debido a que se guardaban las redes como variables de sesión del sitio web, tenían un límite de tamaño impuesto sobre ellas para evitar que algunos navegadores las bloqueasen. Por ello, hubo que modificar una de las redes predefinidas, reduciendo su número de nodos, para así disminuir el tamaño de su versión JSON.
\end{itemize}

\subsection{Diseño del frontend}
Se diseñó un frontend relativamente simple, haciendo uso de Flask para el manejo de endpoints, y Jinja2 para las plantillas HTML. El archivo de Python con la aplicación de Flask gestiona las llamadas a los endpoints que hace el usuario en su navegador web, y prepara la plantilla HTML correspondiente. Haciendo uso de formularios HTML, la aplicación Flask recoge los datos que introduzca el usuario, y llama a los métodos necesarios del backend para preparar el entorno, realizar el entrenamiento y buscar la ruta óptima.

\section{Realización de pruebas}
Una vez desarrollado el algoritmo, se procedió a crear varias pruebas para comprobar su funcionamiento, e intentar arreglar aquellos posibles problemas que surgieran, u optimizar su ejecución.

\subsection{Pruebas unitarias}
Por un lado, se realizó un conjunto de pruebas unitarias sobre los archivos del código fuente. Se creó un fichero de tests unitarios para cada uno de los archivos de código, y se realizaron pruebas sobre todos los métodos existentes en dichos archivos, consistiendo en un total de 59 casos de prueba. 

Para ello se hizo uso del framework de automatización de pruebas Unittest, que permitía la rápida ejecución de los tests después de modificar algún archivo fuente, pudiendo así probar el código una gran cantidad de veces habiendo escrito las pruebas una única vez. La mayoría de entornos de desarrollo tienen funcionalidades o extensiones que facilitan aún más la ejecución de tests y la visualización de los problemas encontrados.

\imagen{tests}{Tests ejecutados en Visual Studio Code}

\subsection{Estudio de valores de entrenamiento}
Por otro lado, también se realizó un estudio de los valores de entrenamiento, para observar la influencia que tenían sobre los resultados obtenidos. Para ello, se definió un conjunto de valores para cada parámetro, y se probó con todas las combinaciones de esos valores en un conjunto de redes de diferentes tamaños.

Para poder comparar el rendimiento de los diferentes experimentos, se decidió registrar cuánto se había aprendido a lo largo de los episodios. Para poder representar esto, se realizaba una búsqueda al finalizar cada episodio, y se guardaba la recompensa obtenida. Al finalizar el entrenamiento se generaba una gráfica con el progreso de las recompensas, pudiendo ver así su variación a lo largo del entrenamiento.

\imagen{graficaExperimento}{Ejemplo de gráfica obtenida al realizar uno de los experimentos}

Sin embargo, debido al proceso de búsqueda de ruta y las diferentes recompensas definidas en el algoritmo, cuando no se encontraba una ruta hasta el nodo objetivo la recompensa obtenida era un valor negativo de una magnitud considerablemente mayor a la recompensa obtenida al encontrar una ruta óptima. Por lo tanto, en las gráficas lo que se observa es un salto muy elevado e instantáneo en el momento en el que se encuentra un camino válido hasta el nodo objetivo. Por ello, se realizó una comparación entre los experimentos del tiempo que se tardaba en encontrar una ruta al nodo objetivo. Detalles sobre los experimentos realizados y sus resultados pueden verse en el Apéndice D.