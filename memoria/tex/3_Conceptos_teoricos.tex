\capitulo{3}{Conceptos teóricos}
Este proyecto utiliza un modelo de aprendizaje automático llamado aprendizaje por refuerzo. Se explican los conceptos más importantes para comprender el proyecto a continuación.

\section{Aprendizaje por refuerzo}

\subsection{Conceptos básicos}

A diferencia de otros modelos de aprendizaje automático, el aprendizaje por refuerzo no es supervisado, pues no cuenta con conjuntos de entrenamiento en los cuales se indican las respuestas correctas a diferentes situaciones, ni tampoco es no supervisado, ya que sí cuenta con recompensas que le informan sobre como de aceptable es una solución concreta. 

Se encuentra en un punto intermedio entre esas dos ramas: Se cuenta con un agente, un programa que decide qué acciones tomar y cuyo propósito es aprender. A este agente, al realizar acciones, se le ofrecen recompensas a corto y largo plazo, de modo que intentará maximizar el valor obtenido con las recompensas, descubriendo qué acciones son óptimas en cada momento.

\subsubsection{Componentes del Aprendizaje por Refuerzo}
Para poder resolver un problema de Aprendizaje por Refuerzo necesitaremos varios elementos esenciales. El primero de ellos será un \textbf{agente}, que como ha sido explicado anteriormente, es quien realiza el aprendizaje, 

Por otro lado, también será necesario un \textbf{entorno} en el cual se realice el propio aprendizaje. Este entorno se representará mediante un conjunto de \textbf{estados} en los que puede estar el agente, y un conjunto de \textbf{acciones} que podrá tomar en cada uno de los diferentes estados.


El agente además contará con una \textbf{política}. Esta política representará el comportamiento del agente, indicando de algún modo qué acciones tomará dependiendo de en qué estado se encuentre en cada momento, con el objetivo de maximizar una recompensa.

Esta señal de \textbf{recompensa} también es un elemento indispensable del problema, siendo uno de los componentes que más influyan sobre los resultados finales. Cada vez que el agente cambie de estado, o cuando pase una unidad de tiempo, se le enviará un valor numérico de recompensa, que indicará como de óptima es la acción tomada si se pretende llegar al objetivo. Por ello, si se diseñan dos algoritmos de Aprendizaje por Refuerzo que tengan todos sus componentes idénticos excepto sus funciones de recompensa, se verá como los agentes pueden llegar a cumplir objetivos completamente diferentes.

Otra parte que está muy fuertemente relacionada con la señal de recompensa será la señal de \textbf{valor} (o calidad). Al igual que las señales de recompensa indican como de buena es la acción o el estado actual, una señal de valor indicará la posible recompensa futura a partir de dicho estado. Esta señal es importante, pues permitirá comportamientos en los que el agente ignora recompensas altas inmediatas, para obtener recompensas mayores en un futuro. Sin embargo, el valor de un estado no es algo que se pueda saber al principio, por lo que es algo que tendrá que ir calculando y aprendiendo el agente a medida que progrese. 

Por ejemplo, en el ejemplo siguiente (Figuras X y X), tenemos una red simple, siendo el inicio el nodo 0 y el objetivo el nodo 2. Después de realizar un entrenamiento hemos obtenido una tabla con valores de calidad para cada pareja estado-acción (siendo el estado el nodo en el que se encuentre el agente, y la acción moverse al nodo indicado o infectar el nodo actual).Se puede ver como, empezando en el estado 0, y seleccionando la acción con un valor más alto de calidad, la ruta óptima será: \verb|0 -> 1 -> 2 -> Infectar| en vez de otras rutas más largas.

\imagen{ejemploRedSimple}{Ejemplo de red simple para demostración de valor de calidad}
\imagen{tablaQ}{Ejemplo de tabla de calidades para la red anterior}

Por último, en algunas situaciones, se puede contar con un \textbf{modelo}. Este modelo consistirá en conocimiento, total o parcial, del entorno de aprendizaje. Esto ayudará al agente a predecir situaciones futuras, y poder tomar decisiones más óptimas con más frecuencia. Sin embargo, no siempre es posible contar con este modelo, por lo que es un elemento opcional.

\subsubsection{Exploración y explotación}

Un tema a tener en cuenta al realizar algoritmos de Aprendizaje por Refuerzo es la necesidad de balancear la exploración y la explotación. Al iniciar el aprendizaje, el agente no tendrá información sobre los valores y recompensas de los diferentes estados, por lo que tendrá que probar acciones e ir recogiendo los valores que se devuelven para ir descubriendo esas recompensas y valores. A este comportamiento se le llama exploración. 
Una vez explorado el entorno, el agente podrá seleccionar las acciones que le devuelvan mayor recompensa o valor, pues son las que más aumentarán la recompensa total. Este otro comportamiento se denomina explotación.

Sin embargo, aunque parezca que, una vez explorado inicialmente el entorno ya no sea necesario volver a hacerlo, y sea más óptimo realizar explotación durante el resto del tiempo para maximizar la recompensa, no siempre será verdad este caso. Es muy probable que no se hayan explorado todos los posibles comportamientos, o que la calidad de algún estado no sea cercana a la realidad al haber sido explorado pocas veces, lo cual resultaría en situaciones en las que alguno de los caminos sin recorrer acabe otorgando mayor recompensa. También existen problemas de aprendizaje por refuerzo en los que el entorno va cambiando a lo largo del tiempo, en los cuales sería esencial realizar exploración a lo largo de todo el proceso de aprendizaje. Por lo tanto, será esencial mantener un equilibrio entre ambos tipos de comportamiento.

\subsection{Procesos de decisión de Markov}

Existen varios tipos de problemas que se pueden resolver mediante aprendizaje por refuerzo, pero el que se va a tratar en este trabajo es un Proceso de Decisión de Markov (MDP).

En un MDP, el agente y el entorno se comunican de forma cíclica.
Al empezar, el entorno le proporcionará al agente una representación de su situación actual, la cual llamaremos estado. Dicho estado se puede entender como el conjunto de características observables por el agente. Con esta información, el agente realizará una acción, notificando al entorno de ello. Al ocurrir dicha acción, el estado del entorno cambiará, siendo enviado al agente junto con una recompensa numérica.

\imagen{entorno-agente}{Relación entre el entorno y el agente en un proceso de decisión de Markov \cite{RLBook}}

La característica principal de los Procesos de Decisión de Markov es que sus estados cumplen la propiedad de Markov. Dicha propiedad es\cite{DeepMind}:
\[P[S_{t}+1|S_{t}] = P[S_{t}+1|S_{1},S_{2},...,S_{t}]\]
Lo cual representa que la probabilidad de llegar a un estado desde un estado anterior es igual a la probabilidad de llegar a dicho estado teniendo en cuenta el camino completo recorrido hasta su estado anterior. Es decir, que cada estado tiene toda la información relevante para tomar decisiones, y no es necesario mirar a los estados anteriores para seleccionar una acción.

Los MDP pueden ser tanto estocásticos como deterministas. En los procesos deterministas, realizar una acción a partir de un estado llevará siempre al mismo estado siguiente, con la misma recompensa asociada. Mientras tanto, en un modelo estocástico, realizar una acción a partir de un estado nos podrá llevar a varios estados destino, cada uno con una recompensa y probabilidad asociada. La suma de las probabilidades de los diferentes estados destino para una pareja estado-acción siempre será 1. En nuestro caso, el problema se representará mediante un Proceso de Decisión de Markov determinista

\subsection{Aprendizaje de diferencia temporal}

En el método de aprendizaje por refuerzo de diferencia temporal, el objetivo es obtener una estimación acertada del valor de cada estado, es decir, la posible recompensa futura que se podrá obtener si se visita dicho estado. 

Un ejemplo del cálculo de la estimación del valor de un estado (llamado $V(S_{t})$) será el siguiente\cite{RLBook}:

\[V(S_{t}) = V(S_{t}) + \alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})]\] 

Lo cual indica que, dado un estado, y una estimación anterior de dicho estado, el incremento de valor entre la nueva estimación y la antigua consistirá en la diferencia entre la suma de la recompensa del estado inmediatamente posterior $R_{t+1}$ y su valor $V(S_{t+1})$ (multiplicado por una tasa de descuento $\gamma$ ) y la estimación del valor del estado actual $V(S_{t})$, multiplicado por la tasa de aprendizaje $\alpha$.

Interpretando esta ecuación paso a paso, podemos ver como $R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})$ será la diferencia entre dós términos:
\begin{itemize}
    \item La estimación del valor actual $V(S_{t})$
    \item Una nueva estimación $R_{t+1} + \gamma V(S_{t+1})$, que se considera más exacta.
\end{itemize}
Podemos afirmar que la segunda será más exacta porque, aunque no podamos saber con certeza el valor de un estado, sí podemos descubrir con exactitud la recompensa después de realizar una acción, y si tenemos en cuenta que el valor de un estado es una estimación de todas las recompensas futuras, al saber una de ellas, estaremos más cerca del valor real. Por lo tanto, podemos considerar $R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})$ como una aproximación al error entre el valor real y el valor estimado, el cual toma el nombre de \textit{error TD}. 

Conociendo este error TD, podremos acercar nuestra estimación al valor real, y eso es lo que hace la operación mostrada anteriormente: suma al valor estimado el error TD, después de haberlo multiplicado por una tasa de aprendizaje $\alpha$.

Este método de aprendizaje proporciona ventajas en comparación a otros, siendo la ventaja principal el hecho de que calcula las estimaciones de los valores basándose en otras estimaciones, y por lo tanto, tiene que esperar menos tiempo para actualizar sus valores, y estos cambiarán con más frecuencia, lo que hará que converjan a los valores reales más rápidamente. En comparación, otros métodos como el aprendizaje Monte Carlo tienen que esperar a que, dentro del entrenamiento, se alcance el objetivo para poder actualizar sus valores.

Otra ventaja es el uso del factor de descuento $\gamma$, que permite definir la importancia que tienen los estados y situaciones futuras en la toma de decisiones. A valores de $\gamma$ muy bajos, se tomará como valor estimado principalmente la recompensa inmediata, haciendo que el valor futuro influya muy poco en el resultado.

\subsubsection{Q learning}

Sin embargo, en la mayoría de sistemas, un estado tendrá varias acciones disponibles, que a su vez llevarán a diferentes estados. Por lo tanto, obtener el valor de un estado concreto no será muy útil, pues nos indicará por qué estados hay que pasar, pero no como se puede llegar a ellos. Considerando esto, se puede ver como es mejor calcular el valor para cada pareja estado-acción, es decir, $V(S_{t}, A_{t})$. De este modo, habrá que modificar el cálculo del valor, y obtendremos la siguiente operación\cite{RLBook}:

\[V(S_{t}, A_{t}) = V(S_{t}, A_{t}) + \alpha[R_{t+1} + \gamma* max_{a}V(S_{t+1},a) - V(S_{t}, A_{t})]\] 

Como se puede ver, en vez de utilizar el valor del estado siguiente, ya que se necesita una pareja estado-acción, se seleccionará la acción que nos devuelva el mayor valor estimado.

Esta variante del aprendizaje por diferencia temporal es llamada \textbf{Q-Learning}, y es el método utilizado en este proyecto.

\section{Redes de ordenadores}
Este algoritmo de aprendizaje por refuerzo que se ha desarrollado está diseñado para navegar por una red de ordenadores. Por \textbf{red de ordenadores} se entiende a un conjunto de dispositivos informáticos, o computadoras, que son autónomas (es decir, no necesitan estar conectadas a otras computadoras para funcionar) y que están interconectadas, independientemente de la forma que tome esta conexión, ya sea mediante bluetooth, wifi, o directamente a través de cableado.\cite{LibroRedes}

Los dispositivos conectados a las redes de ordenadores pueden ser muy variados, pudiendo convivir ordenadores, servidores, switches, impresoras, firewalls, escáneres, y otra multitud de dispositivos en la misma red. No se va a explicar las diferencias entre los comportamientos de estos dispositivos, ni las características de las diversas conexiones posibles, pues para la realización de este trabajo se han considerado todos los dispositivos como iguales, y todas las conexiones como idénticas. Se deja la ampliación de complejidad del entorno como una posible línea de trabajo futura.

Para la organización de todos estos dispositivos, la práctica más común consiste en distribuirlos en una topología de red. Algunos ejemplos de topologías son la \textbf{topología en anillo}, en la cual cada equipo se conecta únicamente con otros dos dispositivos, formando un bucle; la \textbf{topología en estrella}, que cuenta con un dispositivo central al que se conectan todos los demás; o la \textbf{topología en árbol}, que sigue una distribución jerárquica.

\imagen{topologiasRed}{Principales topologías de redes informáticas, obtenido de \cite{topologiasRed}}

En la página web desarrollada se hace uso de una topología mixta, contando con varias redes en forma de árbol, cuyas raíces forman parte de una malla. Esto se decidió así pues, en la mayor parte de situaciones reales en organizaciones o empresas, se cuenta con dispositivos conectados en estructuras jerárquicas, teniendo varios dispositivos conectados a un mismo switch o router, o varios switches conectados a un switch central, por ejemplo. Se incluyó la topología en malla para ofrecer más rutas al agente que una topología en árbol pura, y a la vez simular la redundancia de dispositivos y conexiones que es común en redes grandes. Sin embargo, el agente funciona igualmente aplicado a redes de otras topologías.

\imagen{ejemploRedCompleja}{Ejemplo de red utilizada en el proyecto}