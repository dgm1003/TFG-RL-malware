\apendice{Especificación de diseño}

\section{Introducción}
En este apéndice se describen las consideraciones de diseño que se han tomado a la hora de crear un programa que cumpliese los requisitos tratados en el anexo anterior. Se explican las diferentes estructuras de datos del programa, su relación entre ellas, los procedimientos que realizan, y su distribución dentro de los paquetes del proyecto.

\section{Diseño de datos}
En esta sección se tratarán las consideraciones que se han tomado al representar y guardar los datos del algoritmo. Esto ha cambiado a lo largo de las diferentes versiones, pero solamente se va a considerar la versión final.

Para el almacenamiento de los datos en este proyecto, no se ha considerado necesaria una base de datos. Por lo tanto, todos los datos se almacenan en ficheros persistentes, o Estructuras de Datos dentro de los objetos que se van instanciando. Se explican a continuación estas estructuras.

\subsection{Diseño de datos en el algoritmo}
El algoritmo ha sido dividido en una clase de entorno y otra clase de agente. En los siguientes apartados se explicarán las diferentes estructuras de datos que forman parte de estas dos clases.

\subsubsection{Entorno}
Para la representación del entorno, se han utilizado los siguientes atributos de clase: 

\begin{itemize}
    \item \textbf{Red:} la red de ordenadores se representa mediante un grafo de NetworkX. Los nodos corresponden con los diferentes dispositivos de la red, y además, cada nodo contiene los siguientes atributos:
    \begin{itemize}
        \item \textbf{Riesgo:} un valor numérico indicando el riesgo de detección que tendrá el agente malware si pasa por ese nodo. Su valor mínimo es 1.
        \item \textbf{Infectado:} valor booleano, indica si el nodo está infectado o no.
        \item \textbf{Color:} este atributo lo tienen tanto los nodos como los enlaces. Permite mostrar información al renderizar el entorno, pues los nodos inicial y meta tendrán colores específicos, y aquellos nodos y conexiones por los que haya pasado el agente malware, o que haya infectado, pasarán a tener un color diferente.
    \end{itemize}
    \item \textbf{Número de nodos:} como indica su nombre, corresponde al número de nodos de la red. Es utilizado a la hora de generar el entorno, y ha sido designado como el número correspondiente a la acción de infectar el nodo actual (ya que no habrá nodos con ese número asignado a ellos). 
    \item \textbf{Meta:} valor numérico correspondiente al nodo objetivo.
    \item \textbf{Inicial:} valor numérico correspondiente al nodo origen. Es necesario guardarlo para poder resetear la red.
    \item \textbf{Estado actual:} valor numérico que registra en qué nodo se encuentra el malware en el instante de tiempo actual.
    \item \textbf{Tiempo límite:} número que indica el máximo de acciones que se pueden tomar antes de que se considere como finalizado el objetivo. Impide que un agente se quede tomando acciones sin llegar a la meta de forma infinita, pues de este modo, si no llega antes de un tiempo concreto, no se permiten más movimientos. Se ha definido como el doble del número de nodos, permitiendo al malware recorrer la red completa más de una vez hasta llegar a la meta. Es un valor muy pesimista, y se podría reducir, pero se ha dejado como está para tener una mayor seguridad de que se encuentra la ruta óptima.
\end{itemize}
Originalmente las recompensas para cada una de las acciones se guardaban en una matriz, pero más adelante se pasó a utilizar una función que calculaba la recompensa, para evitar problemas con redes muy grandes.

\subsubsection{Agente}

El agente contendrá dos elementos: Por un lado, tendrá una referencia al objeto del entorno sobre el cual se va a realizar el entrenamiento. Por otro lado, el agente también posee una \textbf{tabla Q}, correspondiente a una matriz en la que se guardan los valores de recompensa futura predecidos para cada par de estado-acción. Esta matriz tendrá un tamaño de $N*N+1$, siendo N el número de nodos de la red.

\subsection{Diseño de datos en la página web}

En el sitio web participa también una tercera clase, aquella que contiene los métodos para los endpoints de Flask. Esta clase utiliza variables de sesión para guardar los diferentes valores -- como los datos obtenidos en formularios -- sin perderlos al cambiar de página.

Sin embargo, uno de los elementos que se necesita que persistan entre las diferentes páginas del sitio web es el propio entorno, debido a la aleatoriedad con la que se genera. Para conseguir esta persistencia, ya que no se permitía guardar objetos de cualquier tipo en las variables de sesión en Flask, se optó por crear unos métodos que permitiesen transformar al entorno en un objeto JSON, compatible con las sesiones de Flask, y transformar al entorno de vuelta a su clase original a partir de un fichero JSON.

Por lo tanto, el entorno se transforma en un diccionario con las diferentes variables que lo forman, incluyendo la red, y a continuación se traduce a formato JSON. Debido a las limitaciones de tamaño que imponen algunos navegadores web sobre las variables de sesión, este método de almacenamiento del entorno tiene la consecuencia de que la red de ordenadores tendrá un límite de, aproximadamente, 150 dispositivos conectados. A efectos del proyecto actual no se ha considerado un problema, pues es un tamaño suficiente para probar el algoritmo, y si se desea ejecutarlo sobre redes más grandes se tiene acceso al código original exento de esta limitación, pero si se desea comercializar o publicar la página web, se deberá modificar este método de almacenamiento de datos.

Además, al generar representaciones gráficas del entorno, se ha añadido la opción de guardarlo como imagen, en unas carpetas concretas, con nombres predefinidos. De este modo, después de pedir al entorno dicha representación, el frontend es capaz de buscar en las rutas definidas y mostrar en la página web el estado del entorno en un momento determinado.

\subsection{Diagrama de clases}

La relación entre las tres clases explicadas anteriormente es la siguiente:

\imagen{DiagramaClases}{Diagrama de clases}




\section{Diseño procedimental}
En esta sección se explican los diferentes procedimientos que forman parte del proyecto, y los componentes que a su vez tienen esos procedimientos. 


Para poder hacer uso de un algoritmo de aprendizaje por refuerzo, es necesario tener un conjunto de estados, acciones y recompensas. En el proyecto actual, se han definido los siguientes:


\subsection{Acciones}
Las acciones que el agente podrá tomar serían:
\begin{itemize}
    \item Desplazarse a otro nodo conectado al actual
    \item Intentar infectar el nodo actual
\end{itemize}
    
De este modo, un agente puede infectar un nodo, moverse sin infectarlo, o moverse después de haberlo infectado.

En caso de pasar a un modelo estocástico, se indicó la posibilidad de añadir probabilidades a la acción de infectar, de modo que pudiese resultar en un equipo infectado o no dependiendo de lo bien defendido que estuviera dicho equipo. No se consideró para el problema actual.

\subsection{Estados}
Los dos valores más importantes de un estado son la posición del malware dentro de la red y la situación del nodo en el que se encuentra, que puede ser infectado o no infectado. Además de esos dos valores, también se tiene en cuenta el tiempo transcurrido desde que el malware se introdujo en el nodo inicial, y el estado de la red, es decir, el resto de nodos que han sido infectados. Por lo tanto, y junto con las acciones definidas anteriormentes, el entorno pasará de un estado a otro diferente cuando el malware se mueva a un nodo adyacente, o infecte el nodo actual, y esa acción se verá reflejada en el nuevo estado.

En caso de ampliar este problema a un proceso de decisión de Markov con un modelo estocástico, se indicó la posibilidad de añadir una tercera situación a los nodos: "detectado", que dificultaría el movimiento del malware, y podría ocurrir con diferentes probabilidades en cada nodo. Sin embargo, esto se consideró fuera del alcance del trabajo.

\subsection{Recompensas}

Se ha definido un conjunto de recompensas inicial, con el cual se considera que se puede llegar al nodo objetivo de una manera eficiente. Se le ha otorgado un valor numérico a cada una de las recompensas, pero esos valores indicando la relación entre los pesos de las diferentes recompensas, y pueden ser cambiados más adelante si se cree que será beneficioso para la eficiencia del algoritmo.
    
\begin{itemize}
    \item \textbf{(-1) a cada segundo que pasa sin haber llegado a la meta:} de este modo se promueve que el malware llegue a su destino lo más rápido posible.
    \item \textbf{(+999) al infectar el nodo objetivo:} consideramos que ha cumplido su meta entonces, por lo que le otorgamos una recompensa elevada.
    \item \textbf{(-5) al infectar un nodo diferente al objetivo:} esto aumentaría la probabilidad de que se detecte que está ocurriendo un ataque en la red, por lo que mientras se tenga un objetivo concreto no es recomendable.
    \item \textbf{(-3) en nodos hoja no objetivos:} tendrá que retroceder para seguir explorando, por lo que se consideran nodos con poco aporte de información, que es preferible evitar.
    \item \textbf{(-10) en nodos de alto riesgo:} aumentan la probabilidad de que los propietarios de la red detecten al malware, por lo que es preferible recorrer el mínimo número posible. 
    \item \textbf{(-111) al moverse de un nodo a otro si no están directamente conectados:} es algo que no debería ser posible, por lo que se penaliza gravemente.
\end{itemize}

Dado este conjunto de recompensas, se ha definido una función que, dado el estado actual y la acción que se desee tomar, calcula la recompensa obtenida.

\subsection{Procesos de entrenamiento y búsqueda de ruta}
Para el entrenamiento del algoritmo, se ha utilizado el método de Tablas Q con diferencia temporal, explicado más a fondo en el capítulo 3.1 de la memoria. 

El algoritmo mantiene una tabla Q, en la cual cada una de sus celdas corresponde a la recompensa futura (llamada valor) esperada al realizar una acción encontrándose en un estado concreto. Se ha optado por un entrenamiento por episodios, de modo que en cada episodio se iniciará el entorno con el malware en la posición de origen, e irá realizando acciones de manera aleatoria hasta llegar a su objetivo. Después de realizar cada acción actualizará la tabla Q, y una vez termine un episodio, reiniciará el entorno y comenzará el siguiente.

En cuanto a la búsqueda, partiendo del nodo de origen, el algoritmo tomará aquellas acciones que, según la tabla Q, prometan un mayor valor. De este modo, si el entrenamiento se ha realizado correctamente, encontrará la ruta óptima hasta el nodo objetivo.

\subsection{Diagramas de secuencia}
A continuación se muestran los diagramas de secuencia para los procesos de creación de la red introduciendo valores, entrenamiento del algoritmo y búsqueda de ruta óptima. Siempre ocurrirán los procesos en ese orden, pero se han separado para una mayor legibilidad.

No se muestra la secuencia de acciones para la creación de entorno mediante selección de modelos predefinidos, pues simplemente consiste en que el usuario pulsa el botón de la red que desea, y el Endpoint pide al constructor del entorno que genere la red seleccionada.

\imagen{DiagramaSecuenciaCreaRed}{Diagrama de secuencia del proceso de creación del entorno}

\imagen{DiagramaSecuenciaEntrenamiento}{Diagrama de secuencia del proceso de entrenamiento y búsqueda de ruta}

\section{Diseño arquitectónico}
En esta sección se explican las consideraciones tomadas al diseñar la arquitectura del proyecto.

\subsection{Arquitectura del algoritmo}
Para poder tener una mayor flexibilidad y posibilidad de reutilización del código, se ha decidido dividir el algoritmo en dos clases: una contiene el entorno completo y todas las operaciones relacionadas con modificar dicho entorno, y la otra contiene el agente de aprendizaje por refuerzo, que se encarga de realizar el entrenamiento y buscar rutas óptimas.

Para que haya compatibilidad entre entorno y agente, el entorno deberá ser una clase con las siguientes funciones:
\begin{itemize}
    \item \verb|step()|: dado el estado actual, realiza una acción. Deberá devolver el nuevo estado del entorno, la recompensa obtenida y un indicador de si ha completado o no su objetivo.
    \item \verb|render()|: genera una representación gráfica del entorno.
    \item \verb|reset()|: reinicia el entorno a sus valores iniciales.
    \item \verb|get_posibles_acciones()|: devuelve una lista con los números asociados a las diferentes acciones que es posible realizar dado un estado concreto.
\end{itemize}

Además, deberá contar con las variables \verb|inicial|, que indica el número del nodo en el que se empieza la ruta, y \verb|NNODOS|, que será equivalente al número de nodos de la red.

Al haber creado esta separación, se podrá utilizar una misma clase de agente para varios tipos de entornos o una misma clase de entorno en varios tipos de agente, con un mínimo de cambios.

Los tests unitarios se incluyeron en un paquete dentro del propio paquete del código fuente, para simplificar la importación del código.

\subsection{Arquitectura del sitio web}
Para la página web, se ha hecho uso de un diseño con separación entre backend y frontend. De este modo, el código del algoritmo se mantiene en el backend, y la representación de la página web en el frontend. Cuando el usuario introduce datos en un formulario o realiza una petición y es necesario hacer uso de las clases del backend, el frontend las llamará para obtener la información necesaria, sin tener que conocer su funcionamiento interno.

La estructura de paquetes es la siguiente:
\imagen{DiagramaPaquetes}{Diagrama de paquetes del sitio web}