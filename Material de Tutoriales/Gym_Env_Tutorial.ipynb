{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcYn7dfoHbJ-",
        "outputId": "ad01c415-9882-4a3e-ffa0-fcff84e51ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable-baselines[mpi]==2.10.0 in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.21.5)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: mpi4py in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->stable-baselines[mpi]==2.10.0) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n"
          ]
        }
      ],
      "source": [
        "# ESTE ARCHIVO ESTÁ HECHO PARA EJECUTARSE EN GOOGLE COLAB\n",
        "# Tutorial utilizado: https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/5_custom_gym_env.ipynb\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "# en vez de Keras en este tutorial se utiliza Stable Baselines\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHahljESUbpD"
      },
      "source": [
        "**Ejemplo de un entorno predefinido**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ6lyCWZHwu0",
        "outputId": "8c0c7c7c-8adf-49a5-d02c-9710b0e10702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Shape: (4,)\n",
            "Action space: Discrete(2)\n",
            "Sampled action: 0\n",
            "(4,) 1.0 False {}\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "# probamos con un entorno de ejemplo\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "# el entorno es una caja, es decir un vector, cuyos valores pueden \n",
        "# estar entre -3.4028234663852886e+38 y 3.4028234663852886e+38\n",
        "print(\"Shape:\", env.observation_space.shape)\n",
        "# el vector tendrá 4 elementos\n",
        "print(\"Action space:\", env.action_space)\n",
        "# tendremos dos acciones (izquierda o derecha)\n",
        "\n",
        "# Llamamos al reset para iniciar el entorno\n",
        "obs = env.reset()\n",
        "# seleccionamos una acción al azar de las posibles\n",
        "action = env.action_space.sample()\n",
        "# realizamos la acción\n",
        "print(\"Sampled action:\", action)\n",
        "obs, reward, done, info = env.step(action)\n",
        "# obtendremos el espacio de observación nuevo, la recompensa de la acción,\n",
        "# si ha terminado o no el episodio, y un diccionario con información \n",
        "# adicional (en este caso está vacío)\n",
        "print(obs.shape, reward, done, info)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAT-mx0cUXVU"
      },
      "source": [
        "**Construcción de nuestro entorno**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "r_Q0l-eIHxFl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "class GoLeftEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Creamos un entorno en el que la respuesta correcta es ir a la izquierda siempre\n",
        "  \"\"\"\n",
        "  # Como esta práctica ha sido realizada en google colab, solo render por consola\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Definimos las constantes para que sea más fácil leer\n",
        "  LEFT = 0\n",
        "  RIGHT = 1\n",
        "\n",
        "  # Creamos el constructor\n",
        "  def __init__(self, grid_size=10):\n",
        "    # empezando llamando al constructor genérico de entornos gym\n",
        "    super(GoLeftEnv, self).__init__()\n",
        "\n",
        "    # después definimos el tamaño de la rejilla por la que se moverá el agente\n",
        "    self.grid_size = grid_size\n",
        "    # Y inicializamos la posición del agente a la derecha del todo\n",
        "    self.agent_pos = grid_size - 1\n",
        "\n",
        "    # Definimos las acciones. En este caso será un espacio discreto (R o L)\n",
        "    n_actions = 2\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    # El espacio de observación será un vector con la posición del agente\n",
        "    # se podría definir también como un espacio discreto\n",
        "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
        "                                        shape=(1,), dtype=np.float32)\n",
        "\n",
        "  # función de reinicio del agente\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # Volvemos a colocar la posición del agente en la posición inicial\n",
        "    self.agent_pos = self.grid_size - 1\n",
        "    # Devolvemos un array con la posición. Se devuelve un array en float32\n",
        "    # aunque sea solo un número sin decimales porque así se ha definido\n",
        "    # al crear el entorno de observación.\n",
        "    return np.array([self.agent_pos]).astype(np.float32)\n",
        "\n",
        "  # función que realiza una acción\n",
        "  def step(self, action):\n",
        "    # actualizamos la posición del agente\n",
        "    if action == self.LEFT:\n",
        "      self.agent_pos -= 1\n",
        "    elif action == self.RIGHT:\n",
        "      self.agent_pos += 1\n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "\n",
        "    # Ajustamos la posición en caso de que se salga del entorno\n",
        "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
        "\n",
        "    # Comprobamos si ha llegado al borde izquierdo\n",
        "    done = bool(self.agent_pos == 0)\n",
        "\n",
        "    # La recompensa será 1 si está en el borde izquierdo y 0 en caso contrario\n",
        "    reward = 1 if self.agent_pos == 0 else 0\n",
        "\n",
        "    # No tenemos más información que devolver.\n",
        "    info = {}\n",
        "\n",
        "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\n",
        "\n",
        "  # definimos la función de renderizar el entorno\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # el agente será una x, el resto de posiciones un .\n",
        "    print(\".\" * self.agent_pos, end=\"\")\n",
        "    print(\"x\", end=\"\")\n",
        "    print(\".\" * (self.grid_size - self.agent_pos))\n",
        "\n",
        "  # función de cerrado una vez se deje de usar el entorno. No es necesaria en este caso.\n",
        "  def close(self):\n",
        "    pass\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "b59mjbrlHxJD"
      },
      "outputs": [],
      "source": [
        "from stable_baselines.common.env_checker import check_env\n",
        "\n",
        "# utilizamos la librería de Stable Baselines para comprobar que está bien construido el entorno.\n",
        "env = GoLeftEnv()\n",
        "check_env(env, warn=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7vU2aWlUT9v"
      },
      "source": [
        "**Prueba del entorno**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLMaGDUSTqiM",
        "outputId": "558a7fae-ce51-4973-ef89-72471f4777a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".........x.\n",
            "Box(0.0, 10.0, (1,), float32)\n",
            "Discrete(2)\n",
            "0\n",
            "Step 1\n",
            "obs= [8.] reward= 0 done= False\n",
            "........x..\n",
            "Step 2\n",
            "obs= [7.] reward= 0 done= False\n",
            ".......x...\n",
            "Step 3\n",
            "obs= [6.] reward= 0 done= False\n",
            "......x....\n",
            "Step 4\n",
            "obs= [5.] reward= 0 done= False\n",
            ".....x.....\n",
            "Step 5\n",
            "obs= [4.] reward= 0 done= False\n",
            "....x......\n",
            "Step 6\n",
            "obs= [3.] reward= 0 done= False\n",
            "...x.......\n",
            "Step 7\n",
            "obs= [2.] reward= 0 done= False\n",
            "..x........\n",
            "Step 8\n",
            "obs= [1.] reward= 0 done= False\n",
            ".x.........\n",
            "Step 9\n",
            "obs= [0.] reward= 1 done= True\n",
            "x..........\n",
            "Goal reached! reward= 1\n"
          ]
        }
      ],
      "source": [
        "# Creamos el entorno\n",
        "env = GoLeftEnv(grid_size=10)\n",
        "\n",
        "# lo reiniciamos y mostramos su estado\n",
        "obs = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "# probamos a ir siempre a la izquierda (agente óptimo)\n",
        "GO_LEFT = 0\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  # a cada paso, ejecutamos la acción e imprimimos sus resultados\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  obs, reward, done, info = env.step(GO_LEFT)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render()\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRwPE12eUPq1"
      },
      "source": [
        "**Entrenado con Stable Baselines**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ep-jXhAHTqsb"
      },
      "outputs": [],
      "source": [
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "\n",
        "# iniciamos el entorno\n",
        "env = GoLeftEnv(grid_size=10)\n",
        "# lo incluímos en un vector\n",
        "env = make_vec_env(lambda: env, n_envs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtm15b12Us3A",
        "outputId": "2d182668-1bc1-4125-b344-6d57c8db3781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| explained_variance | -0.378   |\n",
            "| fps                | 13       |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 0.693    |\n",
            "| policy_loss        | 0.0268   |\n",
            "| total_timesteps    | 20       |\n",
            "| value_loss         | 0.00315  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 35.6     |\n",
            "| ep_reward_mean     | 1        |\n",
            "| explained_variance | -6.1     |\n",
            "| fps                | 412      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 0.236    |\n",
            "| policy_loss        | -0.00812 |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 0.0208   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 10       |\n",
            "| ep_reward_mean     | 1        |\n",
            "| explained_variance | 0.913    |\n",
            "| fps                | 573      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 0.203    |\n",
            "| policy_loss        | -0.00195 |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 5.28e-05 |\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Entrenamos el agente con el método \"Actor Critic using \n",
        "# Kronecker-Factored Trust Region\", con 5000 iteraciones\n",
        "model = ACKTR('MlpPolicy', env, verbose=1).learn(5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ_TjAB6UtOX",
        "outputId": "e679fd4b-4c7f-4d30-9e94-0f15a21a1d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1\n",
            "Action:  [0]\n",
            "obs= [[8.]] reward= [0.] done= [False]\n",
            "........x..\n",
            "Step 2\n",
            "Action:  [0]\n",
            "obs= [[7.]] reward= [0.] done= [False]\n",
            ".......x...\n",
            "Step 3\n",
            "Action:  [0]\n",
            "obs= [[6.]] reward= [0.] done= [False]\n",
            "......x....\n",
            "Step 4\n",
            "Action:  [0]\n",
            "obs= [[5.]] reward= [0.] done= [False]\n",
            ".....x.....\n",
            "Step 5\n",
            "Action:  [0]\n",
            "obs= [[4.]] reward= [0.] done= [False]\n",
            "....x......\n",
            "Step 6\n",
            "Action:  [0]\n",
            "obs= [[3.]] reward= [0.] done= [False]\n",
            "...x.......\n",
            "Step 7\n",
            "Action:  [0]\n",
            "obs= [[2.]] reward= [0.] done= [False]\n",
            "..x........\n",
            "Step 8\n",
            "Action:  [0]\n",
            "obs= [[1.]] reward= [0.] done= [False]\n",
            ".x.........\n",
            "Step 9\n",
            "Action:  [0]\n",
            "obs= [[9.]] reward= [1.] done= [ True]\n",
            ".........x.\n",
            "Goal reached! reward= [1.]\n"
          ]
        }
      ],
      "source": [
        "# comprobamos su funcionamiento\n",
        "obs = env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render(mode='console')\n",
        "  if done:\n",
        "    # Al imprimir el último paso, el entorno se ha reseteado ya automáticamente\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Gym_Env.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f9fd3ee1fa5d99d1b808c165d4ce8152f8cc315b74c23ac2631cf3ab3a2a5937"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
